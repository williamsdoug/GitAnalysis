{
 "metadata": {
  "name": "",
  "signature": "sha256:54c8281db04544f8cf240a43724829f63dd12d536b9bc3e409f19c9e204d49d1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Window-based Blame Analysis for OpenStack Projects\n",
      "\n",
      "Copyright Doug Williams - 2014, 2015\n",
      "\n",
      "###Updated: 2/11/2015"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Includes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pprint import pprint\n",
      "from collections import defaultdict\n",
      "\n",
      "import numpy as np\n",
      "import numpy as np\n",
      "import math\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.cross_validation import StratifiedKFold\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "from sklearn.cross_validation import ShuffleSplit\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn import metrics\n",
      "\n",
      "# from sklearn.feature_extraction import DictVectorizer\n",
      "# from sklearn.preprocessing import StandardScaler\n",
      "#from sklearn.preprocessing import MinMaxScaler\n",
      "# from sklearn.svm import SVR\n",
      "\n",
      "import sys\n",
      "sys.path.append('./dev')\n",
      "\n",
      "from Git_Extract_Join import  filter_bug_fix_combined_commits\n",
      "\n",
      "from commit_analysis import load_core_analysis_data\n",
      "from commit_analysis import fit_features\n",
      "from commit_analysis import extract_features\n",
      "from commit_analysis import compute_guilt\n",
      "from commit_analysis import autoset_threshold\n",
      "\n",
      "# from commit_analysis import blame_compute_normalized_guilt\n",
      "# from commit_analysis import normalize_blame_by_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Configuration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# PROJECT = 'nova'\n",
      "# PROJECT = 'swift'\n",
      "# PROJECT = 'cinder'\n",
      "# PROJECT = 'heat'\n",
      "PROJECT = 'glance'\n",
      "\n",
      "# IMPORTANCE = 'high+'\n",
      "IMPORTANCE = 'med+'\n",
      "\n",
      "PARAM_C = 0.4913\n",
      "PARAM_GAMMA = 0.289"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def eval_clf(clf, X, Y, verbose=True, title=False):\n",
      "    Y_predict = clf.predict(X)\n",
      "    \n",
      "    f1 = metrics.f1_score(Y, Y_predict)\n",
      "    accuracy = metrics.accuracy_score(Y, Y_predict)\n",
      "    precision = metrics.precision_score(Y, Y_predict)\n",
      "    recall = metrics.recall_score(Y, Y_predict)\n",
      "    confusion = metrics.confusion_matrix(Y, Y_predict)\n",
      "    \n",
      "    if verbose:\n",
      "        if title:\n",
      "            print title\n",
      "            print\n",
      "        print 'F1:', f1\n",
      "        print 'accuracy:', accuracy\n",
      "        print 'precision:', precision\n",
      "        print 'recall:', recall\n",
      "        print 'confusion matrix'\n",
      "        print  confusion\n",
      "        print\n",
      "        print metrics.classification_report(Y, Y_predict)\n",
      "    \n",
      "    return {'f1': f1, 'accuracy':accuracy,\n",
      "            'precision': precision, 'recall': recall,\n",
      "            'confusion': confusion}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Preprocessing"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "combined_commits, all_blame = load_core_analysis_data(PROJECT)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "combined_commits: 4076\n",
        "all blame:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1715\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Validate number of Bug Fix related commits"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bug_fix_commits = [k for k,v in combined_commits.items()\n",
      "                   if filter_bug_fix_combined_commits(v, importance=IMPORTANCE)]\n",
      "\n",
      "bug_fix_commits2 = [be['cid'] for be in all_blame\n",
      "                    if filter_bug_fix_combined_commits(combined_commits[be['cid']], \n",
      "                                                       importance=IMPORTANCE)]\n",
      "actual_bugs = len(bug_fix_commits)\n",
      "print 'Bug Fix Commits:', len(bug_fix_commits), '(method 1)', len(bug_fix_commits), '(method 2)', "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Bug Fix Commits: 889 (method 1) 889 (method 2)\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Compute Guilt and Select Threshold Value"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "compute_guilt(combined_commits, all_blame, importance=IMPORTANCE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "entries with non-zero guilt:  1581 out of 4076 ( 38.7880274779 % )\n",
        "smallest guilt: 7.84867749784e-05\n",
        "largest guilt: 15.4417301135\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "guilt_threshold, labeled_bugs = autoset_threshold(combined_commits, actual_bugs)\n",
      "print 'Setting guilt threshold to:', guilt_threshold\n",
      "print 'Labeled bugs:', labeled_bugs, ' vs Actual bugs:', actual_bugs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Setting guilt threshold to: 0.0702935164496\n",
        "Labeled bugs: 889  vs Actual bugs: 889\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extract_state = fit_features(combined_commits, all_blame)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classification"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sliding Window Prediction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def window_predict(combined_commits, all_blame, guilt_threshold,\n",
      "                   offset, history=500, future=100,\n",
      "                   C=1.0, gamma=0.0,\n",
      "                   tune_threshold=False, tune_threshold_granularity=1):\n",
      "    # get training set based on history\n",
      "    cid, Y_t, X_t, col_names = extract_features(combined_commits, all_blame,\n",
      "                                                extract_state, \n",
      "                                                min_order=offset-history, max_order=offset-1,\n",
      "                                                threshold=guilt_threshold, debug=False)\n",
      "    # get features for future\n",
      "    _, Y_f, X_f, _ = extract_features(combined_commits, all_blame,\n",
      "                                      extract_state, \n",
      "                                      min_order=offset, max_order=offset+future-1,\n",
      "                                      threshold=guilt_threshold, debug=False)\n",
      "    \n",
      "    clf = SVC(C=C, gamma=gamma, class_weight='auto')\n",
      "    clf.fit(X_t, Y_t)    # train model based on historical data\n",
      "    \n",
      "\n",
      "    if tune_threshold:\n",
      "        # use training set to optimize decision boundary threshold setting\n",
      "        probs = clf.decision_function(X_t)\n",
      "        best_f1 = -1.0\n",
      "        best_threshold = -1\n",
      "        for i in range(-200, 201, tune_threshold_granularity):\n",
      "            threshold = float(i)/1000.0\n",
      "            predict = probs >= threshold        \n",
      "            f1 = metrics.f1_score(Y_t, predict)\n",
      "        \n",
      "            if f1 > best_f1:\n",
      "                best_f1 = f1\n",
      "                best_threshold = threshold\n",
      " \n",
      "        print 'Using threshold:', best_threshold\n",
      "        probs = clf.decision_function(X_f)\n",
      "        predict_future = probs >= best_threshold        \n",
      "    else:\n",
      "        predict_future = clf.predict(X_f)\n",
      "\n",
      "    # Return quality of future predictions       \n",
      "    cm = metrics.confusion_matrix(Y_f, predict_future)    \n",
      "    return cm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_multiple_windows(combined_commits, all_blame, guilt_threshold,\n",
      "                          starting_offset=0, ending_offset=0, iter=2,\n",
      "                          min_history=100, max_history=1000, future=100,\n",
      "                          C=1.0, gamma=0.0,\n",
      "                          tune_threshold=False, tune_threshold_granularity=1):\n",
      "    min_order = min([c['order'] for c in combined_commits.values()])\n",
      "    max_order = max([c['order'] for c in combined_commits.values()])\n",
      "    \n",
      "    lower = min_order + starting_offset\n",
      "    upper = max_order - future + 1 - ending_offset\n",
      "    \n",
      "    result = np.array([[0,0],[0,0]])\n",
      "    for i in np.random.randint(lower, upper, size=iter):\n",
      "        cm =  window_predict(combined_commits, all_blame, guilt_threshold, i, \n",
      "                             C=C, gamma=gamma,      \n",
      "                             tune_threshold=tune_threshold,\n",
      "                             tune_threshold_granularity=tune_threshold_granularity)\n",
      "        result += cm\n",
      "        \n",
      "    precision = float(result[1,1]) / float(result[0,1] + result[1,1])\n",
      "    recall = float(result[1,1]) / float(result[1,0] + result[1,1])\n",
      "    f1 = (2.0*precision*recall)/(precision+recall)\n",
      "        \n",
      "    return {'f1': f1, 'precision': precision, 'recall': recall, 'cm': result}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_multiple_windows(combined_commits, all_blame, guilt_threshold, iter=10,\n",
      "                         C=PARAM_C, gamma=PARAM_GAMMA)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "{'cm': array([[636, 160],\n",
        "        [135,  69]]),\n",
        " 'f1': 0.31870669745958424,\n",
        " 'precision': 0.30131004366812225,\n",
        " 'recall': 0.3382352941176471}"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_multiple_windows(combined_commits, all_blame, guilt_threshold, iter=10, tune_threshold=True,\n",
      "                         C=PARAM_C, gamma=PARAM_GAMMA)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using threshold: 0.187\n",
        "Using threshold:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.123\n",
        "Using threshold:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.198\n",
        "Using threshold:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.153\n",
        "Using threshold:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.2\n",
        "Using threshold:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.167\n",
        "Using threshold:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.032\n",
        "Using threshold:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.197\n",
        "Using threshold:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.039\n",
        "Using threshold:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.183\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "{'cm': array([[668, 129],\n",
        "        [131,  72]]),\n",
        " 'f1': 0.3564356435643565,\n",
        " 'precision': 0.3582089552238806,\n",
        " 'recall': 0.35467980295566504}"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Boneyard"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Can we use below code to compute a bias value to maximize F1 ?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bar(clf, X, Y, threshold=0.5):\n",
      "    print 'Input Labels'\n",
      "    print metrics.confusion_matrix(Y, Y)\n",
      "    print\n",
      "    print 'Baseline F1:', \n",
      "    predict = clf.predict(X)       \n",
      "    f1 = metrics.f1_score(Y, predict)\n",
      "    print f1\n",
      "    print metrics.confusion_matrix(Y, predict)\n",
      "    print\n",
      "    probs = clf.decision_function(X)\n",
      "    #f1 = metrics.f1_score(Y, y_predict)\n",
      "    \n",
      "    best_f1 = -1.0\n",
      "    best_threshold = -1\n",
      "    for i in range(-200, 201, 1):\n",
      "        threshold = float(i)/1000.0\n",
      "        predict = probs >= threshold        \n",
      "        f1 = metrics.f1_score(Y, predict)\n",
      "        \n",
      "        if f1 > best_f1:\n",
      "            best_f1 = f1\n",
      "            best_threshold = threshold\n",
      "        \n",
      "    threshold = best_threshold\n",
      "    predict = probs >= threshold        \n",
      "    f1 = metrics.f1_score(Y, predict)\n",
      "        \n",
      "    print threshold, ':', \n",
      "    print f1\n",
      "    print metrics.confusion_matrix(Y, predict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bar(clf, X_predict_vshort, Y_predict_vshort)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'clf' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-15-011ec43014e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_predict_vshort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_predict_vshort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def foo(clf, X, Y, threshold=0.5):\n",
      "    print clf.classes_\n",
      "    print\n",
      "    predict = clf.predict(X)\n",
      "    probs = clf.predict_proba(X)\n",
      "    \n",
      "    for i in range(Y.size):\n",
      "        if Y[i] != predict[i]:\n",
      "            print Y[i], predict[i], probs[i][0] - probs[i][1], '     ', probs[i]\n",
      "        \n",
      "    print '------------------------------'\n",
      "    \n",
      "    for i in range(Y.size):\n",
      "        if Y[i] == predict[i]:\n",
      "            print Y[i], predict[i], probs[i][0] - probs[i][1], '     ', probs[i]\n",
      "        \n",
      "    print '------------------------------'\n",
      "    \n",
      "    for i in range(Y.size):\n",
      "        print probs[i][0] + probs[i][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "foo(clf_prob, X_predict_vshort, Y_predict_vshort)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "clf_prob.predict_log_proba(X_predict_vshort)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LinearSVC?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SVC?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}